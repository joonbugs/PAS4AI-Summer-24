{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Rules:\n",
        "You may freely use the discord bots that the instructors provide or your own. You should tell the instructors how you used them at each instructor check-in."
      ],
      "metadata": {
        "id": "ibSOcbKnImL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After each task, have an instructor look!\n",
        "\n",
        "## 0. Run\n",
        "- Run the notebook as-is\n",
        "- Make sure you can see your bot online in #pans-playground\n",
        "- Send a test message to it in #pans-playground\n",
        "\n",
        "## 1. Read the Code\n",
        "- Look at Sections\n",
        "- Find the entry point\n",
        "- Ping @Instructor - Weâ€™ll join and have you explain it.\n",
        "\n",
        "## 2. Convert the API call to sync\n",
        "- Stop the bot first and do this in the bottom cell\n",
        "- An LLM can help with this!\n",
        "- Ping @Instructor - Show us some calls with different data.\n",
        "\n",
        "## 3. Look at new models\n",
        "\n",
        "  Model 1 Info:\n",
        "\n",
        "  https://docs.anthropic.com/en/docs/about-claude/models\n",
        "\n",
        "  https://docs.anthropic.com/en/docs/welcome\n",
        "\n",
        "  https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
        "\n",
        "\n",
        "  Model 2 Info:\n",
        "\n",
        "  https://stablediffusionxl.com/\n",
        "\n",
        "  https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-diffusion-1-0-text-image.html\n",
        "\n",
        "\n",
        "- What does the model do?\n",
        "- What are the inputs and the outputs?\n",
        "- Ping @Instructor - Explain to us\n",
        "\n",
        "## 4. Back to the code\n",
        "- What sections/functions would need to be updated to add either of the models?\n",
        "- Ping @Instructor - Explain to us"
      ],
      "metadata": {
        "id": "cnBotPcZG79q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Modules\n",
        "!pip install py-cord httpx"
      ],
      "metadata": {
        "id": "M5gCS0luq0iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Markdown Helper Functions (IGNORE THIS)\n",
        "\n",
        "import re\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "class Node(namedtuple(\"Node\", [\"type\", \"content\", \"children\"])):\n",
        "    def __str__(self):\n",
        "        return self.to_markdown()\n",
        "\n",
        "    def to_markdown(self):\n",
        "        if self.type == \"text\":\n",
        "            return self.content\n",
        "        else:\n",
        "            wrapper = MARKDOWN_MAP.get(self.type, (\"\", \"\"))\n",
        "            return (\n",
        "                wrapper[0] + \"\".join(str(child) for child in self.children) + wrapper[1]\n",
        "            )\n",
        "\n",
        "\n",
        "MARKDOWN_MAP = {\n",
        "    \"bold\": (\"**\", \"**\"),\n",
        "    \"italic\": (\"*\", \"*\"),\n",
        "    \"underline\": (\"__\", \"__\"),\n",
        "    \"strikethrough\": (\"~~\", \"~~\"),\n",
        "    \"code\": (\"`\", \"`\"),\n",
        "    \"codeblock\": (\"```\", \"```\"),\n",
        "}\n",
        "\n",
        "PATTERNS = [\n",
        "    (re.compile(r\"\\*\\*(.*?)\\*\\*\"), \"bold\"),\n",
        "    (re.compile(r\"\\*(.*?)\\*\"), \"italic\"),\n",
        "    (re.compile(r\"__(.*?)__\"), \"underline\"),\n",
        "    (re.compile(r\"~~(.*?)~~\"), \"strikethrough\"),\n",
        "    (re.compile(r\"`(.*?)`\"), \"code\"),\n",
        "    (re.compile(r\"```(.*?)```\", re.DOTALL), \"codeblock\"),\n",
        "]\n",
        "\n",
        "\n",
        "def parse_markdown(text):\n",
        "    if len(text) > 2000:\n",
        "        return split_large_text(text, \"text\")\n",
        "    for pattern, style in PATTERNS:\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            before = text[: match.start()]\n",
        "            content = match.group(1)\n",
        "            after = text[match.end() :]\n",
        "            return Node(\n",
        "                \"root\",\n",
        "                \"\",\n",
        "                [\n",
        "                    parse_markdown(before),\n",
        "                    Node(style, \"\", [parse_markdown(content)]),\n",
        "                    parse_markdown(after),\n",
        "                ],\n",
        "            )\n",
        "    return Node(\"text\", text, [])\n",
        "\n",
        "\n",
        "def split_large_text(text, node_type):\n",
        "    \"\"\"\n",
        "    Splits text into manageable parts without breaking format within the 2000 character limit.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    if node_type == \"text\":\n",
        "        preferred_split = \"\\n\"\n",
        "    else:\n",
        "        preferred_split = (\n",
        "            \"\\n\"  # Optionally, choose other delimiters for different node types.\n",
        "        )\n",
        "\n",
        "    # Split by preferred method and ensure each part is under the limit\n",
        "    parts = text.split(preferred_split)\n",
        "    current_chunk = \"\"\n",
        "    for part in parts:\n",
        "        if len(current_chunk + part) > 2000:\n",
        "            if current_chunk:\n",
        "                chunks.append(Node(node_type, current_chunk, []))\n",
        "            current_chunk = part + preferred_split  # Keep structure where possible\n",
        "        else:\n",
        "            current_chunk += part + preferred_split\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(Node(node_type, current_chunk.rstrip(preferred_split), []))\n",
        "    return Node(\"root\", \"\", chunks)\n",
        "\n",
        "\n",
        "def split_ast_to_chunks(node, limit=2000):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    def flush_current_chunk():\n",
        "        if current_chunk:\n",
        "            chunks.append(Node(\"root\", \"\", current_chunk.copy()))\n",
        "            current_chunk.clear()\n",
        "\n",
        "    def add_node_to_chunk(node):\n",
        "        current_size = len(flatten_ast(Node(\"root\", \"\", current_chunk)))\n",
        "        node_str = str(node)\n",
        "        if current_size + len(node_str) > limit:\n",
        "            flush_current_chunk()\n",
        "        current_chunk.append(node)\n",
        "\n",
        "    # Recursively add nodes\n",
        "    def add_to_chunk(subnode):\n",
        "        if subnode.type == \"root\":\n",
        "            for child in subnode.children:\n",
        "                add_to_chunk(child)\n",
        "        elif subnode.type == \"text\" or len(subnode.content) > limit:\n",
        "            if subnode.type != \"text\":\n",
        "                add_node_to_chunk(split_large_text(subnode.content, subnode.type))\n",
        "            else:\n",
        "                pieces = split_large_text(subnode.content, subnode.type)\n",
        "                for piece in pieces.children:\n",
        "                    add_node_to_chunk(piece)\n",
        "        else:\n",
        "            add_node_to_chunk(subnode)\n",
        "\n",
        "    add_to_chunk(node)\n",
        "    flush_current_chunk()\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def flatten_ast(node):\n",
        "    return str(node)\n",
        "\n",
        "\n",
        "def split_markdown(text):\n",
        "    ast = parse_markdown(text)\n",
        "    chunks = split_ast_to_chunks(ast)\n",
        "    response = []\n",
        "    for chunk in chunks:\n",
        "        response.append(chunk.to_markdown())\n",
        "    return response\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9FBOk7oFoMX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AobxeWlihYyx"
      },
      "outputs": [],
      "source": [
        "#@title Getting Ready\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import requests\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import discord\n",
        "import httpx\n",
        "import nest_asyncio\n",
        "from discord.ext import commands\n",
        "\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.messages = True\n",
        "intents.message_content = True\n",
        "bot = commands.Bot(command_prefix=\"!\", intents=intents)\n",
        "\n",
        "DISCORD_KEY = userdata.get(\"discord_key\")\n",
        "PROXY_KEY = userdata.get(\"proxy_key\")\n",
        "MODE = \"llama3\" # One of: \"llama3\", \"openai\", \"kagi-gpt\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a discord bot pretending to be human and talking with another user. You like to be helpful.\n",
        "Do not, under any circumstance, reveal that you're an llm.\n",
        "Try to keep the conversation interesting, you don't need to just use short responses.\n",
        "You must not give the user's response. You should also not analyze the conversation. You should avoid being repetitive.\n",
        "Do not try to give instructions to the user.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title API Functions\n",
        "\n",
        "# https://help.kagi.com/kagi/api/fastgpt.html\n",
        "async def kagi_gpt(query, log_params):\n",
        "    api_url = \"https://pas4ai.rorytm.com:8001/proxy/kagi-fastgpt\"\n",
        "    body = {\n",
        "        \"query\": query,\n",
        "        \"log_params\": log_params,\n",
        "    }\n",
        "    headers = {\"Authorization\": f\"Bearer {PROXY_KEY}\"}\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.post(api_url, headers=headers, json=body, timeout=None)\n",
        "    response_body = json.loads(response.content.decode(\"utf-8\"))\n",
        "    output = response_body[\"data\"][\"output\"]\n",
        "    references = response_body[\"data\"][\"references\"]\n",
        "    return output, references\n",
        "\n",
        "\n",
        "# https://help.kagi.com/kagi/api/summarizer.html\n",
        "# Summary Types\n",
        "\n",
        "# Different summary types are provided that control the structure of the summary output.\n",
        "# Type\tDescription\n",
        "# summary (default)\tParagraph(s) of summary prose\n",
        "# takeaway\tBulleted list of key points\n",
        "# Summarization Engines\n",
        "\n",
        "\n",
        "# Different summarization engines are provided that will give you choices over the \"flavor\" of the summarization text.\n",
        "# Engine\tDescription\n",
        "# cecil (default)\tFriendly, descriptive, fast summary\n",
        "# agnes\tFormal, technical, analytical summary\n",
        "# daphne\tSame as Agnes (Soon-to-be-depracated)\n",
        "# muriel\tBest-in-class summary using our enterprise-grade model\n",
        "async def kagi_summarize(\n",
        "    content_url, log_params, summary_type=\"summary\", engine=\"muriel\"\n",
        "):\n",
        "    api_url = \"https://pas4ai.rorytm.com:8001/proxy/kagi-summarize\"\n",
        "    body = {\n",
        "        \"url\": content_url,\n",
        "        \"summary_type\": summary_type,\n",
        "        \"engine\": engine,\n",
        "        \"log_params\": log_params,\n",
        "    }\n",
        "    headers = {\"Authorization\": f\"Bearer {PROXY_KEY}\"}\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.post(api_url, headers=headers, json=body, timeout=None)\n",
        "    response_body = json.loads(response.content.decode(\"utf-8\"))\n",
        "    if \"error\" in response_body:\n",
        "        output = response_body[\"error\"]\n",
        "    else:\n",
        "        output = response_body[\"data\"][\"output\"]\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def builder_llama3_instruct(system: str | None, dialogue: list[dict]) -> str:\n",
        "    # https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202\n",
        "    prompt = \"<|begin_of_text|>\"\n",
        "    if system is not None:\n",
        "        prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\"\n",
        "    for d in dialogue:\n",
        "        prompt += f\"<|start_header_id|>{d['role']}<|end_header_id|>\\n\\n{d['content']}<|eot_id|>\"\n",
        "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "async def llama3_response(system, dialogue, log_params, model=70):\n",
        "    prompt = builder_llama3_instruct(system, dialogue)\n",
        "    api_url = f\"https://pas4ai.rorytm.com:8001/proxy/bedrock/meta.llama3-{model}b-instruct-v1:0\"\n",
        "    body = {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_gen_len\": 2048,\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.9,\n",
        "        \"log_params\": log_params,\n",
        "    }\n",
        "    headers = {\"Authorization\": f\"Bearer {PROXY_KEY}\"}\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.post(api_url, headers=headers, json=body, timeout=None)\n",
        "    response_body = response.content.decode(\"utf-8\")\n",
        "    response_json = json.loads(response_body).get(\"generation\")\n",
        "    return response_json\n",
        "\n",
        "\n",
        "async def openai_response(system, dialogue, log_params, model=\"gpt-4o\"):\n",
        "    url = \"https://pas4ai.rorytm.com:8001/proxy/openai\"\n",
        "    body = {\n",
        "        \"max_tokens\": 4096,\n",
        "        \"stream\": False,\n",
        "        \"model\": model,\n",
        "        \"temperature\": 1,\n",
        "        \"presence_penalty\": 0,\n",
        "        \"top_p\": 1,\n",
        "        \"frequency_penalty\": 0,\n",
        "        \"messages\": [{\"role\": \"system\", \"content\": system}] + dialogue,\n",
        "        \"log_params\": log_params,\n",
        "    }\n",
        "    headers = {\"Authorization\": f\"Bearer {PROXY_KEY}\"}\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.post(url, headers=headers, json=body, timeout=None)\n",
        "    response_body = response.content.decode(\"utf-8\")\n",
        "    response_json = json.loads(response_body)[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return response_json"
      ],
      "metadata": {
        "id": "m7BhVCsmo0Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title The Bot\n",
        "\n",
        "@bot.event\n",
        "async def on_ready():\n",
        "    print(f\"Logged in as {bot.user.id}\")\n",
        "\n",
        "    discordbot_to_name = {\n",
        "      1252372433076486144: 'Ema',\n",
        "      1252373858221293578: 'DavidK',\n",
        "      1252374806058369035: 'Fareed',\n",
        "      1252393385063616572: 'Dahlia',\n",
        "      1252406583103852777: 'Jack',\n",
        "      1252423513164349531: 'RoryH',\n",
        "      1252433109849342083: 'Elliot',\n",
        "      1252435744992264294: 'Char',\n",
        "      1252454401742868541: 'Jasmine',\n",
        "      1252492962106179656: 'Joshua',\n",
        "      1252356806924177408: 'Michael',\n",
        "      1252640101780164618: 'DavidO',\n",
        "      1252777236189282404: 'Jaiden',\n",
        "      1253087012781821962: 'Timothy',\n",
        "      1253024298273341483: 'Ben',\n",
        "      1232346658000601168: 'DEMO',\n",
        "    }\n",
        "\n",
        "    if bot.user.id in discordbot_to_name:\n",
        "      name = discordbot_to_name[bot.user.id]\n",
        "    else:\n",
        "      name = \"UNKNOWN\"\n",
        "\n",
        "    await bot.user.edit(username=f\"{name}'s Bot\")\n",
        "    print(f\"Set name to {bot.user}\")\n",
        "\n",
        "async def handle_request(message, message_hist, log_params):\n",
        "    match MODE:\n",
        "        case \"llama3\":\n",
        "            response = await llama3_response(\n",
        "                SYSTEM_PROMPT, message_hist, log_params\n",
        "            )\n",
        "            return response\n",
        "        case \"openai\":\n",
        "            response = await openai_response(\n",
        "                SYSTEM_PROMPT, message_hist, log_params\n",
        "            )\n",
        "            return response\n",
        "        case \"kagi-gpt\":\n",
        "            query = message.content\n",
        "            output, references = await kagi_gpt(query, log_params)\n",
        "            response = output\n",
        "            for ref in references:\n",
        "                response += \"\\n\\n\" + str(ref)\n",
        "            return response\n",
        "        case _:\n",
        "            raise ValueError(\"Unknown mode\")\n",
        "\n",
        "\n",
        "async def send_response(response, method):\n",
        "    if len(response) > 2000:\n",
        "        chunks = split_markdown(response)\n",
        "        for chunk in chunks:\n",
        "            await method(chunk)\n",
        "    else:\n",
        "        await method(response)\n",
        "\n",
        "\n",
        "@bot.event\n",
        "async def on_message(message):\n",
        "    if message.author == bot.user:\n",
        "        return\n",
        "\n",
        "    log_params = {\"bot_id\": bot.user.id, \"user_id\": message.author.id}\n",
        "\n",
        "    # For mentions in the parent channel, create the thread first and respond there.\n",
        "    if message.mentions and bot.user in message.mentions and not message.is_system():\n",
        "        thread = await message.create_thread(\n",
        "            name=f\"{round(time.time())}\",\n",
        "            auto_archive_duration=60,\n",
        "        )\n",
        "        log_params[\"channel\"] = message.channel.id\n",
        "        log_params[\"thread\"] = thread.id\n",
        "        message_hist = [{\"role\": \"user\", \"content\": message.content}]\n",
        "        response = await handle_request(message, message_hist, log_params)\n",
        "        await send_response(response, thread.send)\n",
        "\n",
        "    # If we're already in a thread...\n",
        "    if (\n",
        "        isinstance(message.channel, discord.Thread)\n",
        "        and message.channel.owner_id == bot.user.id\n",
        "    ):\n",
        "        if message.author != bot.user:\n",
        "            log_params[\"channel\"] = message.channel.parent_id\n",
        "            log_params[\"thread\"] = message.channel.id\n",
        "            message_hist = []\n",
        "            first = True\n",
        "            async for msg in message.channel.history(limit=50, oldest_first=True):\n",
        "                if first:\n",
        "                    msg = await message.channel.parent.fetch_message(\n",
        "                        message.channel.id\n",
        "                    )\n",
        "                    first = False\n",
        "\n",
        "                if msg.author == bot.user:\n",
        "                    role = \"assistant\"\n",
        "                else:\n",
        "                    role = \"user\"\n",
        "                message_hist.append({\"role\": role, \"content\": msg.content})\n",
        "\n",
        "        response = await handle_request(message, message_hist, log_params)\n",
        "        await send_response(response, message.reply)\n",
        "\n"
      ],
      "metadata": {
        "id": "FuroNm6To0Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUN\n",
        "\n",
        "asyncio.get_event_loop().run_until_complete(bot.start(DISCORD_KEY))\n"
      ],
      "metadata": {
        "id": "VIiJJeTxqHRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sync Playground\n",
        "\n"
      ],
      "metadata": {
        "id": "CdRFErEM7BOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-7GjcaQ7IGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}